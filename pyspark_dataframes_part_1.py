# -*- coding: utf-8 -*-
"""PySpark DataFrames- Part 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NeEuDa5DQd_4HsOKxkvrm0SGiD4lt5cd
"""

# Topics Covered
# PySpark Dataframe
# Reading The Dataset
# Checking the Datatypes of the Column(Schema)
# Selecting Columns And Indexing
# Check Describe option similar to Pandas
# Adding Columns
# Dropping columns
# Renaming Columns

!pip install pyspark

import pyspark
from pyspark.sql import SparkSession

spark=SparkSession.builder.appName('Dataframe').getOrCreate()

spark

## read the dataset first row 
spark.read.option('header','true').csv('test2.csv')

## read the dataset  all row 
spark.read.option('header','true').csv('test2.csv').show()

df_pyspark=spark.read.option('header','true').csv('test2.csv')
## print schema is like info in pandas
df_pyspark.printSchema()

#It give string to age and experience but it is int what we do

## read the dataset
df_pyspark=spark.read.option('header','true').csv('test2.csv',inferSchema=True)

df_pyspark.printSchema()

#second way

df_pyspark=spark.read.csv('test2.csv',header=True,inferSchema=True)
df_pyspark.show()

### Check the schema
df_pyspark.printSchema()

type(df_pyspark)

df_pyspark.head(3)

df_pyspark.show()

df_pyspark.select('Name').show()

df_pyspark.select(['Name','Experience']).show()

df_pyspark['Name']

df_pyspark.dtypes

df_pyspark.describe().show()

### Adding Columns in data frame
df_pyspark=df_pyspark.withColumn('Experience After 2 year',df_pyspark['Experience']+2)

df_pyspark.show()

### Drop the columns
df_pyspark=df_pyspark.drop('Experience After 2 year')

df_pyspark.show()

### Rename the columns
df_pyspark.withColumnRenamed('Name','New Name').show()



"""# New Section"""

from google.colab import drive
drive.mount('/content/drive')





b